{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08f4e4a8-3985-4f2b-b89b-2756c08c8b59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t* SHELL : /bin/bash\n\t* PIP_NO_INPUT : 1\n\t* SUDO_GID : 0\n\t* PYTHONHASHSEED : 0\n\t* DISABLE_LOCAL_FILESYSTEM : false\n\t* JAVA_HOME : /usr/lib/jvm/zulu8-ca-amd64/jre/\n\t* MLFLOW_PYTHON_EXECUTABLE : /databricks/spark/scripts/mlflow_python.sh\n\t* JAVA_OPTS :  -Djava.io.tmpdir=/local_disk0/tmp -XX:-OmitStackTraceInFastThrow -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true  -Ddatabricks.serviceName=driver-1 -Xms6260m -Xmx6260m -Dspark.ui.port=40001 -Dspark.executor.extraJavaOptions=\"-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true  -Ddatabricks.serviceName=spark-executor-1\" -Dspark.executor.memory=7284m -Dspark.executor.extraClassPath=/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*\n\t* SUDO_COMMAND : /usr/bin/lxc-attach -n 0701-202558-b11a6pix_10_139_64_4 -- env DB_HOME=/databricks CLUSTER_DB_HOME=/databricks bash -l -c bash ${DB_HOME:-/home/ubuntu/databricks}/spark/scripts/start_chauffeur.sh /tmp/chauffeur-env.sh\n\t* SUDO_USER : root\n\t* PWD : /databricks/driver\n\t* LOGNAME : root\n\t* DB_HOME : /databricks\n\t* KOALAS_USAGE_LOGGER : pyspark.databricks.koalas.usage_logger\n\t* PYARROW_IGNORE_TIMEZONE : 1\n\t* container : lxc\n\t* HIVE_HOME : /home/ubuntu/hive-0.9.0-bin\n\t* DRIVER_PID_FILE : /tmp/driver-daemon.pid\n\t* HOME : /root\n\t* DEFAULT_DATABRICKS_ROOT_VIRTUALENV_ENV : /databricks/python3\n\t* LANG : C.UTF-8\n\t* MLFLOW_TRACKING_URI : databricks\n\t* R_LIBS : /databricks/spark/R/lib:/local_disk0/.ephemeral_nfs/cluster_libraries/r\n\t* VIRTUAL_ENV : /local_disk0/.ephemeral_nfs/envs/pythonEnv-b1807a53-bc2b-4a68-884f-d3ad454876ef\n\t* DATABRICKS_CLUSTER_LIBS_R_ROOT_DIR : r\n\t* CLUSTER_DB_HOME : /databricks\n\t* PYSPARK_GATEWAY_SECRET : f13223c0a60f2fc9b24117f935a3db445f324a933a7c7edbe3d8c4f4e3d1bf3e\n\t* DATABRICKS_RUNTIME_VERSION : 12.2\n\t* PYSPARK_PYTHON : /databricks/python/bin/python\n\t* DATABRICKS_ROOT_VIRTUALENV_ENV : /databricks/python3\n\t* PYTHONPATH : /databricks/spark/python:/databricks/spark/python/lib/py4j-0.10.9.5-src.zip:/databricks/jars/spark--driver--driver-spark_3.3_2.12_deploy.jar:/WSFS_NOTEBOOK_DIR:/databricks/spark/python:/databricks/python_shell\n\t* TERM : xterm-color\n\t* USER : root\n\t* SPARK_PUBLIC_DNS : 10.139.64.4\n\t* SPARK_LOCAL_DIRS : /local_disk0\n\t* PINNED_THREAD_MODE : true\n\t* SHLVL : 0\n\t* MASTER : local[4]\n\t* SPARK_HOME : /databricks/spark\n\t* SPARK_LOCAL_IP : 10.139.64.4\n\t* MLFLOW_CONDA_HOME : /databricks/conda\n\t* PYSPARK_GATEWAY_PORT : 43233\n\t* MPLBACKEND : AGG\n\t* CLASSPATH : /databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/log4j/driver:/databricks/hive/conf:/databricks/spark/dbconf/hadoop:/databricks/jars/*\n\t* SPARK_CONF_DIR : /databricks/spark/conf\n\t* SPARK_DIST_CLASSPATH : /databricks/spark/dbconf/log4j/driver:/databricks/jars/*\n\t* PS1 : (pythonEnv-b1807a53-bc2b-4a68-884f-d3ad454876ef) \n\t* PYENV_ROOT : /databricks/.pyenv\n\t* ENABLE_IPTABLES : false\n\t* JUPYTER_WIDGETS_ECHO : 1\n\t* DATABRICKS_LIBS_NFS_ROOT_PATH : /local_disk0/.ephemeral_nfs\n\t* SPARK_ENV_LOADED : 1\n\t* DATABRICKS_CLUSTER_LIBS_ROOT_DIR : cluster_libraries\n\t* PATH : /local_disk0/.ephemeral_nfs/envs/pythonEnv-b1807a53-bc2b-4a68-884f-d3ad454876ef/bin:/local_disk0/.ephemeral_nfs/cluster_libraries/python/bin:/databricks/.pyenv/bin:/usr/local/nvidia/bin:/databricks/python3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin\n\t* DATABRICKS_LIBS_NFS_ROOT_DIR : .ephemeral_nfs\n\t* SUDO_UID : 0\n\t* DATABRICKS_CLUSTER_LIBS_PYTHON_ROOT_DIR : python\n\t* SPARK_SCALA_VERSION : 2.12\n\t* MAIL : /var/mail/root\n\t* SCALA_VERSION : 2.10\n\t* LOW_PRIVILEGED_LIBRARY_INSTALLATION_USER : libraries\n\t* OLDPWD : /databricks/chauffeur\n\t* SPARK_WORKER_MEMORY : 9106m\n\t* PYDEVD_USE_FRAME_EVAL : NO\n\t* SPARK_AUTH_SOCKET_TIMEOUT : 15\n\t* SPARK_BUFFER_SIZE : 65536\n\t* CLICOLOR : 1\n\t* PAGER : cat\n\t* GIT_PAGER : cat\n3.9.5 (default, Nov 23 2021, 15:27:38) \n[GCC 9.3.0]\nuname_result(system='Linux', node='0701-202558-b11a6pix-10-139-64-4', release='5.15.0-1038-azure', version='#45~20.04.1-Ubuntu SMP Tue Apr 25 18:45:15 UTC 2023', machine='x86_64')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Before starting a code it's important to know the versions, distributions of the software and the machine and work environment that will be used\n",
    "\"\"\"\n",
    "\n",
    "from os import environ as env\n",
    "from sys import version as python_version\n",
    "from platform import uname as machine_info\n",
    "\n",
    "# Show all environment variables\n",
    "for key, value in env.items():\n",
    "    print(\"\\t*\", key, \":\", value)\n",
    "\n",
    "# Show python version\n",
    "print(python_version)\n",
    "\n",
    "# Show machine information like OS, etc.\n",
    "print(machine_info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e38a7977-9992-4f03-b597-431bf6b73703",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of files in: /dbfs/FileStore/tables\n\t* bi_corp\n\t* calendar.csv\n\t* listings.csv\n\t* listings_summary.csv\n\t* neighbourhoods.csv\n\t* neighbourhoods.geojson\n\t* reviews.csv\n\t* reviews_summary.csv\n/dbfs/FileStore/tables : ['bi_corp', 'calendar.csv', 'listings.csv', 'listings_summary.csv', 'neighbourhoods.csv', 'neighbourhoods.geojson', 'reviews.csv', 'reviews_summary.csv']\n/dbfs/FileStore/tables/bi_corp : ['business', 'common', 'landing', 'staging']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The files were initially uploaded to PATH_TABLES. We need to structure the directories for the lakehouse.\n",
    "And then we move the data to landing.\n",
    "\n",
    "Path structure:\n",
    "/dbfs/FileStore/tables/bi_corp/landing/{table_name}/{YYYY}/{MM}/{DD}\n",
    "/dbfs/FileStore/tables/bi_corp/staging/{table_name}/\n",
    "/dbfs/FileStore/tables/bi_corp/common/{table_name}/\n",
    "/dbfs/FileStore/tables/bi_corp/business/{table_name}/\n",
    "\"\"\"\n",
    "\n",
    "from os import listdir, makedirs\n",
    "from scripts.helper import *\n",
    "\n",
    "LIST_FILES = listdir(TABLES_PATH)\n",
    "CSV_FILES = [f for f in LIST_FILES if f.endswith('.csv')]\n",
    "GEO_JSON_FILES = [f for f in LIST_FILES if f.endswith('.geojson')]\n",
    "\n",
    "print(\"List of files in:\", TABLES_PATH)\n",
    "for f in LIST_FILES:\n",
    "    print('\\t*', f)\n",
    "\n",
    "makedirs(LANDING_PATH, exist_ok=True)\n",
    "makedirs(STAGING_PATH, exist_ok=True)\n",
    "makedirs(COMMON_PATH, exist_ok=True)\n",
    "makedirs(BUSINESS_PATH, exist_ok=True)\n",
    "\n",
    "print(TABLES_PATH, ':', listdir(TABLES_PATH))\n",
    "print(BI_CORP_PATH, ':', listdir(BI_CORP_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74b2326c-397a-4045-814f-461a1ee14e8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-02\n/dbfs/FileStore/tables/bi_corp/landing/calendar/2023/07/02 : ['calendar.csv']\n/dbfs/FileStore/tables/bi_corp/landing/listings/2023/07/02 : ['listings.csv']\n/dbfs/FileStore/tables/bi_corp/landing/listings_summary/2023/07/02 : ['listings_summary.csv']\n/dbfs/FileStore/tables/bi_corp/landing/neighbourhoods/2023/07/02 : ['neighbourhoods.csv']\n/dbfs/FileStore/tables/bi_corp/landing/reviews/2023/07/02 : ['reviews.csv']\n/dbfs/FileStore/tables/bi_corp/landing/reviews_summary/2023/07/02 : ['reviews_summary.csv']\n"
     ]
    }
   ],
   "source": [
    "# I need to create one folder per table in the landing zone and move CSV's to it's path\n",
    "\n",
    "from shutil import copy\n",
    "\n",
    "PROCESS_DATE = get_process_date\n",
    "print(\"PROCESS_DATE :\", PROCESS_DATE)\n",
    "\n",
    "for csv_file in CSV_FILES:\n",
    "    name = csv_file.split(\".\")[0]  # get name of the file without extension\n",
    "    makedirs(f'{LANDING_PATH}/{name}/{PROCESS_DATE.replace(\"-\", \"/\")}', exist_ok=True)\n",
    "    file_to_copy = f\"{TABLES_PATH}/{csv_file}\"\n",
    "    target_dir = f'{LANDING_PATH}/{name}/{PROCESS_DATE.replace(\"-\", \"/\")}'\n",
    "\n",
    "    # Copy csv to landing considering process date\n",
    "    try:\n",
    "        # This is commented out to avoid copying files that have already been copied. In the future use move instead of copy.\n",
    "        # copy(file_to_copy, f'{target_dir}/{csv_file}')\n",
    "        pass\n",
    "    except:\n",
    "        print(\"Failed to copy: \", file_to_copy, \"to\", f\"{target_dir}/{csv_file}\")\n",
    "\n",
    "    # Only for debug show if the file was copied\n",
    "    print(target_dir, \":\", listdir(target_dir))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Airbnb_Notebook_00_initialize_lakehouse.ipynb",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
